# -*- coding: utf-8 -*-
"""Loan_Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J0QrSI_Jazdr2P5XLTKiAURwk3S5iJnR
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## Importing the File"""

from google.colab import files

uploaded = files.upload()

import io
loan_data = pd.read_csv(io.BytesIO(uploaded['loan_data_set.csv']))

loan_data.head()

loan_data.drop("Loan_ID",axis=1,inplace=True)

loan_data.info()

loan_data.isnull().sum()

loan_data.dropna(axis=0,inplace=True)

loan_data.isnull().sum()

loan_data.head()

loan_data["Gender"].value_counts()

sns.countplot(x="Gender",data=loan_data,hue="Loan_Status",palette = "Set2")

sns.countplot(x = "Married",data = loan_data,hue="Loan_Status",palette = "Set2")
print(loan_data["Married"].value_counts())

print(loan_data["Education"].value_counts())
sns.countplot(x="Education",data=loan_data,hue="Loan_Status",palette = "Set2")

print(loan_data["Self_Employed"].value_counts())
sns.countplot(x = "Self_Employed",data = loan_data,hue="Loan_Status",palette = "Set2")

loan_data["Property_Area"].value_counts()
sns.countplot(x = "Property_Area", data = loan_data, hue="Loan_Status",palette = "Set2")

plt.figure(figsize=(15,7))
sns.heatmap(loan_data.corr() , annot=True)

loan_data.head()

loan_data["Credit_History"].value_counts()

loan_data["ApplicantIncome"].plot(kind="hist")

loan_data["CoapplicantIncome"].plot(kind="hist")

loan_data["LoanAmount"].plot(kind="hist")

"""## Data Preprocessing"""

Male = pd.get_dummies(loan_data["Gender"],drop_first=True)
Married = pd.get_dummies(loan_data["Married"],drop_first=True)
Education = pd.get_dummies(loan_data["Education"],drop_first=True) #Not graduate
Self_Employed = pd.get_dummies(loan_data["Self_Employed"],drop_first=True)

loan_data["Gender"] = Male
loan_data["Married"] = Married
loan_data["Education"] = Education
loan_data["Self_Employed"] = Self_Employed
loan_data.drop("Education",inplace=True,axis=1)

loan_data.head()

loan_data.head()

from sklearn.preprocessing import StandardScaler

data_to_be_scaled = loan_data[loan_data.columns[4:8]]

scaler = StandardScaler()
scaler.fit(data_to_be_scaled)
scaled_data = scaler.transform(data_to_be_scaled)
scaled_data = pd.DataFrame(scaled_data,columns=loan_data.columns[4:8])
scaled_data.head()

loan_data[loan_data.columns[4:8]] = scaled_data

loan_data.head()

def property_Area(value):
  if value=="Rural":
    return 0
  elif value=="Urban":
    return 1
  elif value=="Semiurban":
    return 2


loan_data["Property_Area"] = loan_data["Property_Area"].apply(lambda x:property_Area(x))

loan_data["Property_Area"].value_counts()

loan_data.head()

plt.figure(figsize=(15,7))
sns.heatmap(loan_data.corr(),annot=True)

loan_data.dropna(axis=0,inplace=True)

loan_data.isnull().sum()

def dependents(value):
  if value=="1":
    return 1
  elif value=="2":
    return 2
  elif value=="0":
    return 0
  else:
    return 3


loan_data["Dependents"] =   loan_data["Dependents"].apply(dependents)

"""## Train Test Split"""

from sklearn.model_selection import train_test_split

X = loan_data.drop("Loan_Status",axis=1)
y = loan_data["Loan_Status"]

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

def encode_labels(value):
  if value=="Y":
    return 1
  else:
    return 0

loan_data["Loan_Status"] = loan_data["Loan_Status"].apply(encode_labels)
loan_data.head()
loan_data.drop(["LOANN??","Clusters"],axis=1,inplace=True)

"""### SVM CLASSIFIER"""

X_train.value_counts()

from sklearn.svm import SVC

svm_model = SVC()
svm_model.fit(X_train, y_train)
preds = svm_model.predict(X_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

print(confusion_matrix(y_test,preds))

print(classification_report(y_test,preds))

print(accuracy_score(y_test,preds))

"""### Accuracy by SVM = 72%

# Logistic Regression Model
"""

from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit(X_train, y_train)
log_preds = log_model.predict(X_test)

print(confusion_matrix(y_test, log_preds))

print(classification_report(y_test, log_preds))

print(accuracy_score(y_test, log_preds))

"""## Accuracy By Logistic Regression = 74.6%

# KNearestNeighbors
"""

from sklearn.neighbors import KNeighborsClassifier

#Optimal K Value
error_rate = []
for i in range(1,40):
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(X_train,y_train)
  preds_i = knn.predict(X_test)
  error_rate.append(np.mean(preds_i != y_test))

#Plotting it
plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue',linestyle='dashed',marker='o',markerfacecolor='red',markersize=10)
plt.title("Error rate vs k value")
plt.xlabel('K')
plt.ylabel('Error Rate')

knn_model = KNeighborsClassifier(n_neighbors=12)
knn_model.fit(X_train,y_train)
knn_preds = knn.predict(X_test)

print(confusion_matrix(y_test, knn_preds))
print(classification_report(y_test, knn_preds))
print(accuracy_score(y_test, knn_preds))

"""### Accuracy By KNN = 68%

# Decission Trees
"""

from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier()
dtree.fit(X_train, y_train)
d_tree_preds = dtree.predict(X_test)

print(confusion_matrix(y_test, d_tree_preds))
print(classification_report(y_test, d_tree_preds))
print(accuracy_score(y_test, d_tree_preds))

"""## Accuracy By Decision Tree = 72%

# Random Forest Model
"""

from sklearn.ensemble import RandomForestClassifier

rfc_model = RandomForestClassifier()
rfc_model.fit(X_train,y_train)
rfc_model_preds = rfc_model.predict(X_test)

print(confusion_matrix(y_test, rfc_model_preds))
print(classification_report(y_test, rfc_model_preds))
print(accuracy_score(y_test, rfc_model_preds))

"""## Random Forest Model = 76%

# KMEANS CLUSTERING - IS IT A UNSUPERVISED DATA?? WE HAVE BEEN WRONG ??
"""

from sklearn.cluster import KMeans

# Finding wcss vvalue for diffrent number of clusters
wcss = []

for i in range(1,11):
  kmeans = KMeans(n_clusters=i, init='k-means++',random_state=42)
  kmeans.fit(X)
  wcss.append(kmeans.inertia_) #inertia will give us WCSS value

#Creating a graph to see which cluster give minimum value
sns.set()
plt.plot(range(1,11), wcss)
plt.title("The Elbow Pointer Graph")
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

kmeans = KMeans(n_clusters=7, init='k-means++',random_state=0)

#return a label for each data point (we have 5 clusters so we need 5 labels)
kmeans_preds = kmeans.fit_predict(X) #returns the cluster number for each point

def converter(private):
    if private == 'Yes':
        return 1
    else:
        return 0

loan_data["Clusters"] = loan_data["Loan_Status"].apply(converter)

print(confusion_matrix(loan_data["Clusters"],kmeans.labels_))
print(classification_report(loan_data["Clusters"],kmeans.labels_))
print(accuracy_score(loan_data["Clusters"],kmeans.labels_))